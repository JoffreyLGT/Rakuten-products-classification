{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Setup : chargement des données et des modèles](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Dataset batch size\n",
    "BATCH_SIZE = 256\n",
    "# Directory containing the dataset pickles\n",
    "DATA_DIR = os.path.join(\"data\", \"pickle_img_datasets\")\n",
    "# Directory containing images\n",
    "IMAGES_DIR = os.path.join(\"data\", \"images\", \"image_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from src.data import data\n",
    "\n",
    "\n",
    "# Load data\n",
    "X_train = pickle.load(\n",
    "    open(os.path.join(DATA_DIR, \"X_train.pkl\"), \"rb\")).fillna(\"\")\n",
    "X_test = pickle.load(\n",
    "    open(os.path.join(DATA_DIR, \"X_test.pkl\"), \"rb\")).fillna(\"\")\n",
    "y_test = pickle.load(open(os.path.join(DATA_DIR, \"y_test.pkl\"), \"rb\"))\n",
    "y_train = pickle.load(open(os.path.join(DATA_DIR, \"y_train.pkl\"), \"rb\"))\n",
    "\n",
    "# Extract the features to be ready for preprocessing\n",
    "X_train_features = X_train['designation'] + \" \" + X_train['description']\n",
    "X_test_features = X_test['designation'] + \" \" + X_test['description']\n",
    "\n",
    "# Store the file path to images in variables\n",
    "X_train_images = data.get_imgs_filenames(\n",
    "    X_train[\"productid\"], X_train[\"imageid\"], IMAGES_DIR)\n",
    "X_test_images = data.get_imgs_filenames(\n",
    "    X_test[\"productid\"], X_test[\"imageid\"], IMAGES_DIR)\n",
    "\n",
    "# Define DataFrame names for preprocessing\n",
    "X_train_features.name = \"X_train\"\n",
    "X_test_features.name = \"X_test\"\n",
    "\n",
    "# Load text model\n",
    "text_model = keras.models.load_model(\n",
    "    os.path.join(\"data\", \"models\", \"mlp_text\", \"mlp_model_v2.1.h5\"), compile=False)\n",
    "\n",
    "# Load image model\n",
    "image_model = keras.models.load_model(\n",
    "    os.path.join(\"data\", \"models\", \"cnn_mobilenetv2_keras\",\n",
    "                 \"cnn_mobilenetv2.h5\"),\n",
    "    compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Preprocessing du texte](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:textpipeline:class:TfidfStemming\n",
      "INFO:textpipeline:TextPreprocess.fit X_train 158.31 seconds\n",
      "INFO:textpipeline:TextPreprocess.transform X_test 36.67 seconds\n",
      "INFO:textpipeline:TextPreprocess.transform X_train 213.52 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.data.text_preproc_pipeline import TextPreprocess\n",
    "from src.data.vectorization_pipeline import TfidfStemming\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "text_preprocessor = TextPreprocess(TfidfStemming())\n",
    "text_preprocessor.fit(X_train_features)\n",
    "X_test_preproc = text_preprocessor.transform(X_test_features)\n",
    "X_train_preproc = text_preprocessor.transform(X_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Prédictions du modèle texte](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Setup : chargement des données et des modèles](#toc1_1_)    \n",
    "- [Preprocessing du texte](#toc2_)    \n",
    "- [Prédictions du modèle texte](#toc3_)    \n",
    "- [Preprocessing des images](#toc4_)    \n",
    "- [Prédictions du modèle image](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 2s 3ms/step\n",
      "Text model accuracy score: 0.8086434291097504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.data.data import convert_sparse_matrix_to_sparse_tensor, get_model_prediction\n",
    "\n",
    "\n",
    "# Predict the categories of X_test\n",
    "y_pred_text = text_model.predict(\n",
    "    convert_sparse_matrix_to_sparse_tensor(X_test_preproc))\n",
    "\n",
    "# Display the accuracy score\n",
    "print(\"Text model accuracy score:\", accuracy_score(\n",
    "    y_test, get_model_prediction(y_pred_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Preprocessing des images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data import PRDTYPECODE_DIC, to_simplified_prdtypecode\n",
    "\n",
    "\n",
    "def open_resize_img(filename: str, y) -> None:\n",
    "    \"\"\"\n",
    "    Open image using the filename and return a resized version of it ready for the image model.\n",
    "\n",
    "    Argument:\n",
    "    - filename: complete path to image file including the extension.\n",
    "\n",
    "    Return:\n",
    "    - Image matrix in a tensor.\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return (tf.image.resize(img, [224, 224]), y)\n",
    "\n",
    "\n",
    "# Convert the prdtypecode to their equivalent in a range from 0 to 26\n",
    "y_test_simplified = to_simplified_prdtypecode(y_test)\n",
    "# Transforms y_test to a one hot version\n",
    "y_test_categorical = tf.keras.utils.to_categorical(\n",
    "    y_test_simplified, num_classes=len(PRDTYPECODE_DIC.keys()))\n",
    "\n",
    "# Create the Dataset to feed the model with correctly sized images by batch\n",
    "test_images_dataset = tf.data.Dataset.from_tensor_slices((X_test_images, y_test_categorical)) \\\n",
    "    .map(open_resize_img) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "# Convert the prdtypecode to their equivalent in a range from 0 to 26\n",
    "y_train_simplified = to_simplified_prdtypecode(y_train)\n",
    "# Transforms y_test to a one hot version\n",
    "y_train_categorical = tf.keras.utils.to_categorical(\n",
    "    y_train_simplified, num_classes=len(PRDTYPECODE_DIC.keys()))\n",
    "\n",
    "# Create the Dataset to feed the model with correctly sized images by batch\n",
    "train_images_dataset = tf.data.Dataset.from_tensor_slices((X_train_images, y_train_categorical)) \\\n",
    "    .map(open_resize_img) \\\n",
    "    .batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédictions du modèle image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 388s 6s/step\n",
      "Image model accuracy score: 0.5444536033914272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.data.data import get_model_prediction\n",
    "\n",
    "\n",
    "# Predict the categories of X_test\n",
    "y_pred_image = image_model.predict(test_images_dataset)\n",
    "\n",
    "# Display the accuracy score\n",
    "print(\"Image model accuracy score:\", accuracy_score(\n",
    "    y_test, get_model_prediction(y_pred_image)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédiction avant dernière couche du modèle texte](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 1s 3ms/step\n",
      "2123/2123 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from src.data.data import convert_sparse_matrix_to_sparse_tensor\n",
    "\n",
    "# text model without head\n",
    "text_model_wo_head = tf.keras.Model(\n",
    "                            inputs=text_model.inputs,\n",
    "                            outputs=text_model.layers[-2].output)\n",
    "\n",
    "# Predict the output n-1 layer of X_test\n",
    "test_text_layer = text_model_wo_head.predict(\n",
    "     convert_sparse_matrix_to_sparse_tensor(X_test_preproc))\n",
    "# Predict the output n-1 layer of X_train\n",
    "train_text_layer = text_model_wo_head.predict(\n",
    "     convert_sparse_matrix_to_sparse_tensor(X_train_preproc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédiction avant dernière couche du modèle image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 371s 6s/step\n",
      "266/266 [==============================] - 1537s 6s/step\n"
     ]
    }
   ],
   "source": [
    "# image model without head\n",
    "image_model_wo_head = tf.keras.Model(inputs=image_model.inputs,\n",
    "                           outputs=image_model.layers[-2].output)\n",
    "\n",
    "# Predict the output n-1 layer with X_test\n",
    "test_image_output = image_model_wo_head.predict(test_images_dataset)\n",
    "\n",
    "# Predict the output n-1 layer with X_train\n",
    "train_image_output = image_model_wo_head.predict(train_images_dataset)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Data concatenation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Text/Image train concatenation \n",
    "train_concat_layer = np.concatenate((train_text_layer, train_image_output), axis=1)\n",
    "\n",
    "filename = os.path.join(DATA_DIR, r'fusion_train_data.pkl')\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(train_concat_layer, fp)\n",
    "\n",
    "# Text/Image train concatenation \n",
    "test_concat_layer = np.concatenate((test_text_layer, test_image_output), axis=1)\n",
    "\n",
    "filename = os.path.join(DATA_DIR, r'fusion_test_data.pkl')\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(test_concat_layer, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_fusion_dataset = tf.data.Dataset.from_tensor_slices((train_concat_layer, y_train_categorical)).batch(BATCH_SIZE)\n",
    "test_fusion_dataset = tf.data.Dataset.from_tensor_slices((test_concat_layer, y_test_categorical)).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Model Fusion definition](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Input/Output dimensions \n",
    "INPUT_FUSION_SIZE = train_concat_layer.shape[1]\n",
    "NB_OF_OUTPUT_CLASSES = 27\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(INPUT_FUSION_SIZE)))\n",
    "model.add(layers.Dense(units=512, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.2, name=\"Dropout\"))\n",
    "\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(NB_OF_OUTPUT_CLASSES, \n",
    "                       activation='softmax',\n",
    "                       name=\"Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint to load\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "\n",
    "\n",
    "# Checkpoint directory and paths\n",
    "CHECKPOINT_DIR = os.path.join(\n",
    "    \"data\", \"models\", \"fusion_text_image_keras\")\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    CHECKPOINT_DIR, \"cp_{val_loss:.2f}-{val_accuracy:.2f}-.ckpt\")\n",
    "\n",
    "# Path to the history CSV file to store training metrics\n",
    "HIST_CSV_PATH = os.path.join(CHECKPOINT_DIR, \"history.csv\")\n",
    "\n",
    "# Define where to store training logs\n",
    "LOG_DIR = os.path.join(CHECKPOINT_DIR, \"logs\", \"fit\")\n",
    "LOG_DATA = os.path.join(\n",
    "    LOG_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# Training\n",
    "model.build((None, INPUT_FUSION_SIZE))\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "latest = latest_checkpoint(CHECKPOINT_DIR)\n",
    "if (latest is not None):\n",
    "    print(\"Loading checkpoint\", latest)\n",
    "    model.load_weights(latest)\n",
    "else:\n",
    "    print(\"No checkpoint to load\")\n",
    "\n",
    "# Callbacks called between each epoch\n",
    "cp_callbacks = [\n",
    "    # Stop the training when there is no improvement in val_accuracy for x epochs\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "    # Save a checkpoint\n",
    "    ModelCheckpoint(CHECKPOINT_PATH,\n",
    "                    save_best_only=True,\n",
    "                    mode=\"max\",\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    save_weights_only=True,\n",
    "                    verbose=1),\n",
    "    # Insert the metrics into a CSV file\n",
    "    CSVLogger(HIST_CSV_PATH, separator=',', append=True),\n",
    "    # Log information to display them in TensorBoard\n",
    "    TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_fusion_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=test_fusion_dataset,\n",
    "    #callbacks=cp_callbacks)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
