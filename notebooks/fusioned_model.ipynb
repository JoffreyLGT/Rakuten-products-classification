{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Setup : chargement des données et des modèles](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Dataset batch size\n",
    "BATCH_SIZE = 256\n",
    "# Directory containing the dataset pickles\n",
    "DATA_DIR = os.path.join(\"data\", \"pickle_img_datasets\")\n",
    "# Directory containing images\n",
    "IMAGES_DIR = os.path.join(\"data\", \"images\", \"image_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from src.data import data\n",
    "\n",
    "\n",
    "# Load data\n",
    "X_train = pickle.load(\n",
    "    open(os.path.join(DATA_DIR, \"X_train.pkl\"), \"rb\")).fillna(\"\")\n",
    "X_test = pickle.load(\n",
    "    open(os.path.join(DATA_DIR, \"X_test.pkl\"), \"rb\")).fillna(\"\")\n",
    "y_test = pickle.load(open(os.path.join(DATA_DIR, \"y_test.pkl\"), \"rb\"))\n",
    "y_train = pickle.load(open(os.path.join(DATA_DIR, \"y_train.pkl\"), \"rb\"))\n",
    "\n",
    "# Extract the features to be ready for preprocessing\n",
    "X_train_features = X_train['designation'] + \" \" + X_train['description']\n",
    "X_test_features = X_test['designation'] + \" \" + X_test['description']\n",
    "\n",
    "# Store the file path to images in variables\n",
    "X_train_images = data.get_imgs_filenames(\n",
    "    X_train[\"productid\"], X_train[\"imageid\"], IMAGES_DIR)\n",
    "X_test_images = data.get_imgs_filenames(\n",
    "    X_test[\"productid\"], X_test[\"imageid\"], IMAGES_DIR)\n",
    "\n",
    "# Define DataFrame names for preprocessing\n",
    "X_train_features.name = \"X_train\"\n",
    "X_test_features.name = \"X_test\"\n",
    "\n",
    "# Load text model\n",
    "text_model = keras.models.load_model(\n",
    "    os.path.join(\"data\", \"models\", \"mlp_text\", \"mlp_model_v2.1.h5\"), compile=False)\n",
    "\n",
    "# Load image model\n",
    "image_model = keras.models.load_model(\n",
    "    os.path.join(\"data\", \"models\", \"cnn_mobilenetv2_keras\",\n",
    "                 \"cnn_mobilenetv2.h5\"),\n",
    "    compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Preprocessing du texte](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:textpipeline:class:TfidfStemming\n",
      "INFO:textpipeline:TextPreprocess.fit X_train 158.31 seconds\n",
      "INFO:textpipeline:TextPreprocess.transform X_test 36.67 seconds\n",
      "INFO:textpipeline:TextPreprocess.transform X_train 213.52 seconds\n"
     ]
    }
   ],
   "source": [
    "from src.data.text_preproc_pipeline import TextPreprocess\n",
    "from src.data.vectorization_pipeline import TfidfStemming\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "text_preprocessor = TextPreprocess(TfidfStemming())\n",
    "text_preprocessor.fit(X_train_features)\n",
    "X_test_preproc = text_preprocessor.transform(X_test_features)\n",
    "X_train_preproc = text_preprocessor.transform(X_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Prédictions du modèle texte](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Setup : chargement des données et des modèles](#toc1_1_)    \n",
    "- [Preprocessing du texte](#toc2_)    \n",
    "- [Prédictions du modèle texte](#toc3_)    \n",
    "- [Preprocessing des images](#toc4_)    \n",
    "- [Prédictions du modèle image](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 2s 3ms/step\n",
      "Text model accuracy score: 0.8086434291097504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.data.data import convert_sparse_matrix_to_sparse_tensor, get_model_prediction\n",
    "\n",
    "\n",
    "# Predict the categories of X_test\n",
    "y_pred_text = text_model.predict(\n",
    "    convert_sparse_matrix_to_sparse_tensor(X_test_preproc))\n",
    "\n",
    "# Display the accuracy score\n",
    "print(\"Text model accuracy score:\", accuracy_score(\n",
    "    y_test, get_model_prediction(y_pred_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Preprocessing des images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data import PRDTYPECODE_DIC, to_simplified_prdtypecode\n",
    "\n",
    "\n",
    "def open_resize_img(filename: str, y) -> None:\n",
    "    \"\"\"\n",
    "    Open image using the filename and return a resized version of it ready for the image model.\n",
    "\n",
    "    Argument:\n",
    "    - filename: complete path to image file including the extension.\n",
    "\n",
    "    Return:\n",
    "    - Image matrix in a tensor.\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return (tf.image.resize(img, [224, 224]), y)\n",
    "\n",
    "\n",
    "# Convert the prdtypecode to their equivalent in a range from 0 to 26\n",
    "y_test_simplified = to_simplified_prdtypecode(y_test)\n",
    "# Transforms y_test to a one hot version\n",
    "y_test_categorical = tf.keras.utils.to_categorical(\n",
    "    y_test_simplified, num_classes=len(PRDTYPECODE_DIC.keys()))\n",
    "\n",
    "# Create the Dataset to feed the model with correctly sized images by batch\n",
    "test_images_dataset = tf.data.Dataset.from_tensor_slices((X_test_images, y_test_categorical)) \\\n",
    "    .map(open_resize_img) \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "# Convert the prdtypecode to their equivalent in a range from 0 to 26\n",
    "y_train_simplified = to_simplified_prdtypecode(y_train)\n",
    "# Transforms y_test to a one hot version\n",
    "y_train_categorical = tf.keras.utils.to_categorical(\n",
    "    y_train_simplified, num_classes=len(PRDTYPECODE_DIC.keys()))\n",
    "\n",
    "# Create the Dataset to feed the model with correctly sized images by batch\n",
    "train_images_dataset = tf.data.Dataset.from_tensor_slices((X_train_images, y_train_categorical)) \\\n",
    "    .map(open_resize_img) \\\n",
    "    .batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédictions du modèle image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 388s 6s/step\n",
      "Image model accuracy score: 0.5444536033914272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from src.data.data import get_model_prediction\n",
    "\n",
    "\n",
    "# Predict the categories of X_test\n",
    "y_pred_image = image_model.predict(test_images_dataset)\n",
    "\n",
    "# Display the accuracy score\n",
    "print(\"Image model accuracy score:\", accuracy_score(\n",
    "    y_test, get_model_prediction(y_pred_image)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédiction avant dernière couche du modèle texte](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 1s 3ms/step\n",
      "2123/2123 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from src.data.data import convert_sparse_matrix_to_sparse_tensor\n",
    "\n",
    "# text model without head\n",
    "text_model_wo_head = tf.keras.Model(\n",
    "                            inputs=text_model.inputs,\n",
    "                            outputs=text_model.layers[-2].output)\n",
    "\n",
    "# Predict the output n-1 layer of X_test\n",
    "test_text_layer = text_model_wo_head.predict(\n",
    "     convert_sparse_matrix_to_sparse_tensor(X_test_preproc))\n",
    "# Predict the output n-1 layer of X_train\n",
    "train_text_layer = text_model_wo_head.predict(\n",
    "     convert_sparse_matrix_to_sparse_tensor(X_train_preproc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prédiction avant dernière couche du modèle image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 371s 6s/step\n",
      "266/266 [==============================] - 1537s 6s/step\n"
     ]
    }
   ],
   "source": [
    "# image model without head\n",
    "image_model_wo_head = tf.keras.Model(inputs=image_model.inputs,\n",
    "                           outputs=image_model.layers[-2].output)\n",
    "\n",
    "# Predict the output n-1 layer with X_test\n",
    "test_image_output = image_model_wo_head.predict(test_images_dataset)\n",
    "\n",
    "# Predict the output n-1 layer with X_train\n",
    "train_image_output = image_model_wo_head.predict(train_images_dataset)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Data concatenation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Text/Image train concatenation \n",
    "train_concat_layer = np.concatenate((train_text_layer, train_image_output), axis=1)\n",
    "\n",
    "filename = os.path.join(DATA_DIR, r'fusion_train_data.pkl')\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(train_concat_layer, fp)\n",
    "\n",
    "# Text/Image train concatenation \n",
    "test_concat_layer = np.concatenate((test_text_layer, test_image_output), axis=1)\n",
    "\n",
    "filename = os.path.join(DATA_DIR, r'fusion_test_data.pkl')\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(test_concat_layer, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_fusion_dataset = tf.data.Dataset.from_tensor_slices((train_concat_layer, y_train_categorical)).batch(BATCH_SIZE)\n",
    "test_fusion_dataset = tf.data.Dataset.from_tensor_slices((test_concat_layer, y_test_categorical)).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Model Fusion definition](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Input/Output dimensions \n",
    "INPUT_FUSION_SIZE = train_concat_layer.shape[1]\n",
    "NB_OF_OUTPUT_CLASSES = 27\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(INPUT_FUSION_SIZE)))\n",
    "model.add(layers.Dense(units=512, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.2, name=\"Dropout\"))\n",
    "\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "outputs = layers.Dense(NB_OF_OUTPUT_CLASSES, \n",
    "                       activation='softmax',\n",
    "                       name=\"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint to load\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 27) and (None, 128) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 54\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Callbacks called between each epoch\u001b[39;00m\n\u001b[0;32m     38\u001b[0m cp_callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m     39\u001b[0m     \u001b[39m# Stop the training when there is no improvement in val_accuracy for x epochs\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     TensorBoard(log_dir\u001b[39m=\u001b[39mLOG_DIR, histogram_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m ]\n\u001b[1;32m---> 54\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     55\u001b[0m     train_fusion_dataset,\n\u001b[0;32m     56\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     57\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtest_fusion_dataset,\n\u001b[0;32m     58\u001b[0m     \u001b[39m#callbacks=cp_callbacks)\u001b[39;49;00m\n\u001b[0;32m     59\u001b[0m )\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewln0pi1t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"d:\\Agnoli\\Datascientest\\Projet\\Fev23_BDS_Rakuten\\.conda\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 27) and (None, 128) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "\n",
    "\n",
    "# Checkpoint directory and paths\n",
    "CHECKPOINT_DIR = os.path.join(\n",
    "    \"data\", \"models\", \"fusion_text_image_keras\")\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    CHECKPOINT_DIR, \"cp_{val_loss:.2f}-{val_accuracy:.2f}-.ckpt\")\n",
    "\n",
    "# Path to the history CSV file to store training metrics\n",
    "HIST_CSV_PATH = os.path.join(CHECKPOINT_DIR, \"history.csv\")\n",
    "\n",
    "# Define where to store training logs\n",
    "LOG_DIR = os.path.join(CHECKPOINT_DIR, \"logs\", \"fit\")\n",
    "LOG_DATA = os.path.join(\n",
    "    LOG_DIR, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# Training\n",
    "model.build((None, INPUT_FUSION_SIZE))\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss=CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "latest = latest_checkpoint(CHECKPOINT_DIR)\n",
    "if (latest is not None):\n",
    "    print(\"Loading checkpoint\", latest)\n",
    "    model.load_weights(latest)\n",
    "else:\n",
    "    print(\"No checkpoint to load\")\n",
    "\n",
    "# Callbacks called between each epoch\n",
    "cp_callbacks = [\n",
    "    # Stop the training when there is no improvement in val_accuracy for x epochs\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "    # Save a checkpoint\n",
    "    ModelCheckpoint(CHECKPOINT_PATH,\n",
    "                    save_best_only=True,\n",
    "                    mode=\"max\",\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    save_weights_only=True,\n",
    "                    verbose=1),\n",
    "    # Insert the metrics into a CSV file\n",
    "    CSVLogger(HIST_CSV_PATH, separator=',', append=True),\n",
    "    # Log information to display them in TensorBoard\n",
    "    TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_fusion_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=test_fusion_dataset,\n",
    "    #callbacks=cp_callbacks)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
