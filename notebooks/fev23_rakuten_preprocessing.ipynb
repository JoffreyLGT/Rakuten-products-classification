{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version de Python utilisée : 3.9.xx\n",
    "# Import des librairies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the raw dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des fichiers contenant les données\n",
    "df = pd.concat(\n",
    "        [pd.read_csv(\"../data/X_train.csv\", index_col=0),\n",
    "        pd.read_csv(\"../data/Y_train.csv\", index_col=0)],\n",
    "        axis=1)\n",
    "#\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "data = df['designation']+\" \"+df['description']\n",
    "data = df['description']\n",
    "\n",
    "target = df['prdtypecode']\n",
    "\n",
    "# remove tags\n",
    "#html_tags = np.unique(np.array(re.findall(\"<[^>]*>\", data.str.cat())))\n",
    "\n",
    "html_tags = [x for y in data.apply(lambda x: np.unique(np.array(re.findall(\"<[^>]*>\", x)))) for x in y ]\n",
    "html_tags = np.unique(html_tags)\n",
    "data.replace(html_tags, \"\", inplace=True)\n",
    "\n",
    "# split to train, valid and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                 Olivia: Personalisiertes Notizbuch / 150 Seiten / Punktraster / Ca Din A5 / Rosen-Design\n",
       "1                                           Journal Des Arts (Le) N° 133 Du 28/09/2001 - L'art Et Son Marche Salon D'art Asiatique A Paris - Jacques Barrere - Francois Perrier - La Reforme Des Ventes Aux Encheres Publiques - Le Sna Fete Ses Cent Ans.\n",
       "2                                                                                                                                                                             Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style\n",
       "3                                                                                                                                                                                          Peluche Donald - Europe - Disneyland 2000 (Marionnette À Doigt)\n",
       "4                                                                                                                                                                                                                                     La Guerre Des Tuques\n",
       "                                                                                                                               ...                                                                                                                        \n",
       "84911                                                                                                                                                                                                                          The Sims [ Import Anglais ]\n",
       "84912                                                                                                                                                                                                 Kit piscine acier NEVADA déco pierre Ø 3.50m x 0.90m\n",
       "84913    Journal Officiel De La Republique Francaise N° 46 Du 15/02/1871 - Changement D'adresses - Partie Officielle - Partie Non Officielle - Elections A L'assemblee Nationale - Ravitaillement De Paris - Nouvelles Etrangeres  -  Italie  -  Amerique.\n",
       "84914                                                                                                                                                                                      Table Basse Bois De Récupération Massif Base Blanche 60x60x33cm\n",
       "84915                                                                                                                                                                                               Gomme De Collection 2 Gommes Pinguin Glace Vert Orange\n",
       "Name: designation, Length: 84916, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing & vectorisation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['etaient', 'etais', 'etait', 'etant', 'etante', 'etantes', 'etants', 'ete', 'etee', 'etees', 'etes', 'etiez', 'etions', 'eumes', 'eutes', 'fumes', 'fur', 'futes', 'konnen', 'konnte', 'meme', 'uber', 'wahrend', 'wurde', 'wurden'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorisation_type = 'tf_idf' # \"bag_of_word\"  # or tf_idf\n",
    "\n",
    "# nltk french and english stop words\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french') + stopwords.words('german')\n",
    "\n",
    "preproc_config_CountVect = {'strip_accents' : 'unicode',\n",
    "                     'lowercase' : True,\n",
    "                     'stop_words' : final_stopwords_list, # got from nltk (french and english)\n",
    "                     'ngram_range' : (1,1),   # The lower and upper boundary of the range of n-values for different word n-grams or char n-grams\n",
    "                     'analyzer' : 'word',     # The feature should be made of word n-gram ('word') or character n-grams (‘char’)\n",
    "                     'max_df' : 1., # the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "                     'min_df' : 0.001, # the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "                     'max_features' : None, #build a vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "                     'vocabulary' : None # Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix or iterable\n",
    "                    }\n",
    "\n",
    "preproc_config_TfidfVect = {'norm' : 'l2',\n",
    "                            'use_idf' : True,\n",
    "                            'smooth_idf' : True,\n",
    "                            'sublinear_tf' : False\n",
    "                            }\n",
    "\n",
    "if vectorisation_type == \"bag_of_word\":\n",
    "    # bag of word\n",
    "    pipe = Pipeline([('count', CountVectorizer(**preproc_config_CountVect))]).fit(X_train.to_list())\n",
    "if vectorisation_type == \"tf_idf\":\n",
    "    # tfidf\n",
    "    pipe = Pipeline([('count', CountVectorizer(**preproc_config_CountVect)),\n",
    "                    ('tfid', TfidfTransformer(**preproc_config_TfidfVect))]).fit(X_train.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pipe['count'].vocabulary_\n",
    "\n",
    "with open('../data/training/voc_tfidf_min_df0_001.pkl','wb') as fp:\n",
    "    pickle.dump(vocabulary, fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipe.transform(X_train.to_list()).toarray()\n",
    "X_train_tfidf = pd.DataFrame(X).fillna(0)\n",
    "\n",
    "#X_train_tfidf.to_pickle(\"../data/training/X_train_tfidf_min_df0_001_description.pkl\")\n",
    "y_train.to_pickle(\"../data/training/y_train_tfidf_min_df0_001_description.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipe.transform(X_test.to_list()).toarray()\n",
    "\n",
    "X_test_tfidf = pd.DataFrame(X).fillna(0)\n",
    "\n",
    "# save X_test\n",
    "#X_test_tfidf.to_pickle(\"../data/training/X_test_tfidf_min_df0_001_description.pkl\")\n",
    "y_test.to_pickle(\"../data/training/y_test_tfidf_min_df0_001_description.pkl\")\n",
    "\n",
    "# view words with tfidf > 0.\n",
    "if 0:\n",
    "    for c in X_test_tfidf.columns:\n",
    "\n",
    "        if X_test_tfidf.head(2)[c].values[0]>0.0:\n",
    "            pos = list(pipe['count'].vocabulary_.values()).index(c)\n",
    "            word = list(pipe['count'].vocabulary_.keys())[pos]\n",
    "            print(word, X_test_tfidf.head(1)[c])\n",
    "del(X_test_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check le corpus après preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6753275"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recuperation de la fonction de preprocessing\n",
    "preproc_ = pipe['count'].build_preprocessor()\n",
    "# sur le dataframe data mais on peut aussi le faire que sur designation ou description\n",
    "data_preproc = preproc_(data.str.cat())\n",
    "# nombre des mots\n",
    "len(data_preproc.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistique sur les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du voc sur la base train: 5778\n",
      "Statistique corpus designation:\n",
      "      nb_chars\n",
      "count  5778.00\n",
      "mean      6.58\n",
      "std       2.62\n",
      "min       2.00\n",
      "25%       5.00\n",
      "50%       6.00\n",
      "75%       8.00\n",
      "max      16.00\n"
     ]
    }
   ],
   "source": [
    "# on peut faire varier 'min_df' dans preproc_config_CountVect : 0 0.001 0.002 0.003 etc.\n",
    "\n",
    "print(\"taille du voc sur la base train:\", len(pipe['count'].vocabulary_.keys()))\n",
    "pipe['count'].vocabulary_.keys()\n",
    "voc_words = pd.DataFrame(pipe['count'].vocabulary_.keys(), columns=['words'])\n",
    "voc_words['nb_chars'] = voc_words.words.apply(lambda x: len(x))\n",
    "\n",
    "print('Statistique corpus designation:')\n",
    "print(voc_words.describe().apply(lambda x: x.apply('{0:<.2f}'.format)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single vectorisation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texte preprocessing\n",
    "\n",
    "voir https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bag of Words representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir un ensemble de document texte dans une matrice d'occurance de token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk french and english stop words\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french') + stopwords.words('german')\n",
    "#final_stopwords_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Au niveau mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn\n",
    "preproc_config_CountVect = {'strip_accents' : 'unicode',\n",
    "                     'lower_case' : True,\n",
    "                     'stop_word' : final_stopwords_list, # got from nltk (french and english)\n",
    "                     'ngram_range' : (1,1),   # The lower and upper boundary of the range of n-values for different word n-grams or char n-grams\n",
    "                     'analyzer' : 'word',     # The feature should be made of word n-gram ('word') or character n-grams (‘char’)\n",
    "                     'max_df' : 1., # the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "                     'min_df' : 0., # the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "                     'max_features' : None, #build a vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "                     'vocabulary' : None # Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix or iterable\n",
    "                    }\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#count_vect.input = df.designation.head(100)\n",
    "count_vect.encoding = 'utf-8'\n",
    "count_vect.strip_accents = preproc_config_CountVect['strip_accents']\n",
    "count_vect.lower_case = preproc_config_CountVect['lower_case']\n",
    "count_vect.stop_word = preproc_config_CountVect['stop_word']\n",
    "count_vect.ngram_range = preproc_config_CountVect['ngram_range']\n",
    "count_vect.analyzer = preproc_config_CountVect['analyzer']\n",
    "count_vect.max_df = preproc_config_CountVect['max_df']\n",
    "count_vect.min_df = preproc_config_CountVect['min_df']\n",
    "count_vect.stop_word = preproc_config_CountVect['stop_word']\n",
    "count_vect.vocabulary = preproc_config_CountVect['vocabulary']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 145969\n",
      "first 10 words from the voc\n",
      "['ramadan', 'eid', 'de', 'luxe', 'lumieres', 'del', 'interieur', 'lanternes', 'or', 'mnl535']\n"
     ]
    }
   ],
   "source": [
    "count_vect.fit(X_train)\n",
    "#X_counts = count_vect.fit_transform()\n",
    "voc = count_vect.vocabulary_\n",
    "print(\"Vocabulary size:\",len(voc))\n",
    "print(\"first 10 words from the voc\")\n",
    "print([k for i, (k,v) in enumerate(zip(voc.keys(),voc.values())) if i<10])\n",
    "\n",
    "#print(\"Token:\")\n",
    "#print(\"nombre de token: \", len(count_vect.get_feature_names_out()))\n",
    "#print(count_vect.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Au niveau N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>145959</th>\n",
       "      <th>145960</th>\n",
       "      <th>145961</th>\n",
       "      <th>145962</th>\n",
       "      <th>145963</th>\n",
       "      <th>145964</th>\n",
       "      <th>145965</th>\n",
       "      <th>145966</th>\n",
       "      <th>145967</th>\n",
       "      <th>145968</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145969 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       1       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   9       ...  145959  145960  145961  145962  145963  145964  145965  \\\n",
       "0       0  ...       0       0       0       0       0       0       0   \n",
       "1       0  ...       0       0       0       0       0       0       0   \n",
       "2       0  ...       0       0       0       0       0       0       0   \n",
       "3       0  ...       0       0       0       0       0       0       0   \n",
       "4       0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "   145966  145967  145968  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       0       0  \n",
       "3       0       0       0  \n",
       "4       0       0       0  \n",
       "\n",
       "[5 rows x 145969 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_count_vect = count_vect.transform(X_train)\n",
    "X_test_count_vect = count_vect.transform(X_test)\n",
    "X_test_count_vect = pd.DataFrame(X_test_count_vect.toarray())\n",
    "                                 \n",
    "X_test_count_vect.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir un ensemble de document brut dans une matrice de feature TF-IDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on dispose déjà de la matrice d'occurence des token nous pouvons utiliser la classe _TfidfTransformer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_config_TfidfVect = {'norm' : 'l2',\n",
    "                            'use_idfbool' : True,\n",
    "                            'smooth_idfbool' : True,\n",
    "                            'sublinear_tf' : False\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.norm = preproc_config_TfidfVect['norm']\n",
    "vectorizer.use_idfbool = preproc_config_TfidfVect['use_idfbool']\n",
    "vectorizer.smooth_idfbool = preproc_config_TfidfVect['smooth_idfbool']\n",
    "vectorizer.sublinear_tf = preproc_config_TfidfVect['sublinear_tf']\n",
    "\n",
    "#X_train = X_train.head(1000)\n",
    "#X_test = X_test.head(1000)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_tfidf = pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bleu'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.values()).index(2720)\n",
    "list(vectorizer.vocabulary_.keys())[338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu 0    0.190921\n",
      "Name: 2720, dtype: float64\n",
      "bluetooth 0    0.326458\n",
      "Name: 2751, dtype: float64\n",
      "dualshock 0    0.326458\n",
      "Name: 4918, dtype: float64\n",
      "fil 0    0.214033\n",
      "Name: 5935, dtype: float64\n",
      "gamepad 0    0.326458\n",
      "Name: 6324, dtype: float64\n",
      "playstation 0    0.285002\n",
      "Name: 9876, dtype: float64\n",
      "pour 0    0.084537\n",
      "Name: 10078, dtype: float64\n",
      "sans 0    0.152748\n",
      "Name: 11380, dtype: float64\n",
      "sony 0    0.616227\n",
      "Name: 11893, dtype: float64\n",
      "v2 0    0.326458\n",
      "Name: 13135, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for c in X_test_tfidf.columns:\n",
    "    #print(type(X_test_tfidf.head(1)[c]))\n",
    "    #print(pd.DataFrame(X_test_tfidf.head(1)[c]))\n",
    "    #print(pd.DataFrame(X_test_tfidf.head(1)[c])==0.)\n",
    "    #print(X_test_tfidf.head(1)[c].values==0.)\n",
    "\n",
    "    if X_test_tfidf.head(1)[c].values[0]>0.0:\n",
    "        pos = list(vectorizer.vocabulary_.values()).index(c)\n",
    "        word = list(vectorizer.vocabulary_.keys())[pos]\n",
    "        print(word, X_test_tfidf.head(1)[c])\n",
    "    #print(X_test_tfidf[X_test_tfidf>0.].head(1)[c])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['etaient', 'etais', 'etait', 'etant', 'etante', 'etantes', 'etants', 'ete', 'etee', 'etees', 'etes', 'etiez', 'etions', 'eumes', 'eutes', 'fumes', 'fur', 'futes', 'konnen', 'konnte', 'meme', 'uber', 'wahrend', 'wurde', 'wurden'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "preproc_config_CountVect = {'strip_accents' : 'unicode',\n",
    "                     'lowercase' : True,\n",
    "                     'stop_words' : final_stopwords_list, # got from nltk (french and english)\n",
    "                     'ngram_range' : (1,1),   # The lower and upper boundary of the range of n-values for different word n-grams or char n-grams\n",
    "                     'analyzer' : 'word',     # The feature should be made of word n-gram ('word') or character n-grams (‘char’)\n",
    "                     'max_df' : 1., # the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "                     'min_df' : 0., # the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "                     'max_features' : None, #build a vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "                     'vocabulary' : None # Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix or iterable\n",
    "                    }\n",
    "\n",
    "preproc_config_TfidfVect = {'norm' : 'l2',\n",
    "                            'use_idf' : True,\n",
    "                            'smooth_idf' : True,\n",
    "                            'sublinear_tf' : False\n",
    "                            }\n",
    "\n",
    "#count_vect = CountVectorizer()\n",
    "#count_vect.input = df.designation.head(100)\n",
    "\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#X_train.str.lower().to_list()[0]\n",
    "pipe = Pipeline([('count', CountVectorizer(**preproc_config_CountVect)),\n",
    "                 ('tfid', TfidfTransformer(**preproc_config_TfidfVect))]).fit(X_train.to_list())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1pc 0    0.063711\n",
      "Name: 7269, dtype: float64\n",
      "30x35cm 0    0.167637\n",
      "Name: 12065, dtype: float64\n",
      "326 0    0.027789\n",
      "Name: 12435, dtype: float64\n",
      "34x35cm 0    0.167637\n",
      "Name: 12897, dtype: float64\n",
      "39 0    0.316589\n",
      "Name: 13782, dtype: float64\n",
      "43 0    0.052511\n",
      "Name: 15092, dtype: float64\n",
      "actionner 0    0.032332\n",
      "Name: 25668, dtype: float64\n",
      "affecter 0    0.059794\n",
      "Name: 26380, dtype: float64\n",
      "aident 0    0.02138\n",
      "Name: 26830, dtype: float64\n",
      "ainsi 0    0.012888\n",
      "Name: 26936, dtype: float64\n",
      "argile 0    0.021019\n",
      "Name: 30593, dtype: float64\n",
      "article 0    0.089787\n",
      "Name: 31002, dtype: float64\n",
      "aussi 0    0.012636\n",
      "Name: 32248, dtype: float64\n",
      "autour 0    0.08503\n",
      "Name: 32519, dtype: float64\n",
      "autres 0    0.147444\n",
      "Name: 32526, dtype: float64\n",
      "avoir 0    0.012546\n",
      "Name: 32759, dtype: float64\n",
      "balcon 0    0.1351\n",
      "Name: 33696, dtype: float64\n",
      "beaucoup 0    0.086232\n",
      "Name: 34935, dtype: float64\n",
      "besoin 0    0.01399\n",
      "Name: 35603, dtype: float64\n",
      "bon 0    0.065428\n",
      "Name: 37365, dtype: float64\n",
      "bonne 0    0.013136\n",
      "Name: 37428, dtype: float64\n",
      "bricolage 0    0.015022\n",
      "Name: 38559, dtype: float64\n",
      "caracteristiques 0    0.045971\n",
      "Name: 41497, dtype: float64\n",
      "carottes 0    0.135238\n",
      "Name: 41779, dtype: float64\n",
      "choc 0    0.018221\n",
      "Name: 44630, dtype: float64\n",
      "conception 0    0.074565\n",
      "Name: 47991, dtype: float64\n",
      "concu 0    0.098375\n",
      "Name: 48085, dtype: float64\n",
      "container 0    0.022878\n",
      "Name: 48876, dtype: float64\n",
      "conteneur 0    0.023448\n",
      "Name: 48924, dtype: float64\n",
      "contenu 0    0.075239\n",
      "Name: 48948, dtype: float64\n",
      "cote 0    0.075195\n",
      "Name: 50032, dtype: float64\n",
      "couleur 0    0.054258\n",
      "Name: 50175, dtype: float64\n",
      "creuser 0    0.025003\n",
      "Name: 50983, dtype: float64\n",
      "croissance 0    0.122346\n",
      "Name: 51183, dtype: float64\n",
      "croissant 0    0.188217\n",
      "Name: 51185, dtype: float64\n",
      "cultivez 0    0.05479\n",
      "Name: 51635, dtype: float64\n",
      "culture 0    0.039493\n",
      "Name: 51643, dtype: float64\n",
      "date 0    0.083804\n",
      "Name: 52733, dtype: float64\n",
      "deplacer 0    0.113287\n",
      "Name: 54791, dtype: float64\n",
      "difference 0    0.084341\n",
      "Name: 56316, dtype: float64\n",
      "differents 0    0.087832\n",
      "Name: 56362, dtype: float64\n",
      "diy 0    0.016516\n",
      "Name: 57324, dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m X_test_tfidf\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m      8\u001b[0m         \u001b[39m#print(type(X_test_tfidf.head(1)[c]))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         \u001b[39m#print(pd.DataFrame(X_test_tfidf.head(1)[c]))\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[39m#print(pd.DataFrame(X_test_tfidf.head(1)[c])==0.)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         \u001b[39m#print(X_test_tfidf.head(1)[c].values==0.)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         \u001b[39mif\u001b[39;00m X_test_tfidf\u001b[39m.\u001b[39;49mhead(\u001b[39m2\u001b[39;49m)[c]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m0.0\u001b[39m:\n\u001b[0;32m     14\u001b[0m             pos \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(pipe[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mvalues())\u001b[39m.\u001b[39mindex(c)\n\u001b[0;32m     15\u001b[0m             word \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(pipe[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mkeys())[pos]\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\generic.py:5547\u001b[0m, in \u001b[0;36mNDFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   5474\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   5475\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhead\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, n: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   5476\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5477\u001b[0m \u001b[39m    Return the first `n` rows.\u001b[39;00m\n\u001b[0;32m   5478\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5545\u001b[0m \u001b[39m    5     parrot\u001b[39;00m\n\u001b[0;32m   5546\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5547\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miloc[:n]\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\indexing.py:1602\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[0;32m   1597\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1598\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using .loc for automatic alignment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1599\u001b[0m     )\n\u001b[0;32m   1601\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[1;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_slice_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   1605\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\indexing.py:1638\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1636\u001b[0m labels \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1637\u001b[0m labels\u001b[39m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[1;32m-> 1638\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_slice(slice_obj, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\generic.py:4106\u001b[0m, in \u001b[0;36mNDFrame._slice\u001b[1;34m(self, slobj, axis)\u001b[0m\n\u001b[0;32m   4104\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_block_manager_axis(axis)\n\u001b[0;32m   4105\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mget_slice(slobj, axis\u001b[39m=\u001b[39maxis))\n\u001b[1;32m-> 4106\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49m__finalize__(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   4108\u001b[0m \u001b[39m# this could be a view\u001b[39;00m\n\u001b[0;32m   4109\u001b[0m \u001b[39m# but only in a single-dtyped view sliceable case\u001b[39;00m\n\u001b[0;32m   4110\u001b[0m is_copy \u001b[39m=\u001b[39m axis \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m result\u001b[39m.\u001b[39m_is_view\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\generic.py:5868\u001b[0m, in \u001b[0;36mNDFrame.__finalize__\u001b[1;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[0;32m   5865\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m other\u001b[39m.\u001b[39mattrs:\n\u001b[0;32m   5866\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrs[name] \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mattrs[name]\n\u001b[1;32m-> 5868\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflags\u001b[39m.\u001b[39;49mallows_duplicate_labels \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mallows_duplicate_labels\n\u001b[0;32m   5869\u001b[0m \u001b[39m# For subclasses using _metadata.\u001b[39;00m\n\u001b[0;32m   5870\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(other\u001b[39m.\u001b[39m_metadata):\n",
      "File \u001b[1;32md:\\Agnoli\\Datascientest\\Formation\\Modules\\.conda\\lib\\site-packages\\pandas\\core\\flags.py:96\u001b[0m, in \u001b[0;36mFlags.allows_duplicate_labels\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39maxes:\n\u001b[0;32m     94\u001b[0m         ax\u001b[39m.\u001b[39m_maybe_check_unique()\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_allows_duplicate_labels \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = pipe.transform(X_test.to_list()).toarray()\n",
    "\n",
    "X_test_tfidf = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "if 1:\n",
    "    for c in X_test_tfidf.columns:\n",
    "        #print(type(X_test_tfidf.head(1)[c]))\n",
    "        #print(pd.DataFrame(X_test_tfidf.head(1)[c]))\n",
    "        #print(pd.DataFrame(X_test_tfidf.head(1)[c])==0.)\n",
    "        #print(X_test_tfidf.head(1)[c].values==0.)\n",
    "\n",
    "        if X_test_tfidf.head(2)[c].values[0]>0.0:\n",
    "            pos = list(pipe['count'].vocabulary_.values()).index(c)\n",
    "            word = list(pipe['count'].vocabulary_.keys())[pos]\n",
    "            print(word, X_test_tfidf.head(1)[c])\n",
    "del(X_test_tfidf)\n",
    "#pipe['count'].vocabulary_.values()\n",
    "#pipe['count'].vocabulary_.keys()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "voir https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectivity graph of an image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
